{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Airline Tweets Sentiment Analysis Lab\n",
    "\n",
    "_Authors: Phillippa Thomson (NYC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to be analyzing tweets about airlines.  These have been hand-tagged with sentiment.  There are three categories: positive, neutral, and negative.\n",
    "\n",
    "Use VADER to calculate sentiment for each tweet, and see if you can correctly predict the hand-tagged sentiment.\n",
    "\n",
    "What is the accuracy?  Print out a heatmap to see where your model performs well, and where it performs poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T14:14:24.381235Z",
     "start_time": "2021-02-22T14:14:21.026473Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, \\\n",
    "precision_score, recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T14:14:24.420583Z",
     "start_time": "2021-02-22T14:14:24.383646Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../../data/Tweets.csv\", encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preview the airline_sentiment column.\n",
    "- What percentage of reviews are positive, neutral, and negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['airline_sentiment'].value_counts() / len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load in the Sentiment IntensityAnalyzer from Vader and add compound, negative, neutral, and positive scores into the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "compound= []\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "for tweet in tweets['text']:\n",
    "    sent = sia.polarity_scores(tweet)\n",
    "    compound.append(sent['compound'])\n",
    "    neg.append(sent['neg'])\n",
    "    neu.append(sent['neu'])\n",
    "    pos.append(sent['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['compound'] = compound\n",
    "tweets['neg'] = neg\n",
    "tweets['neu'] = neu\n",
    "tweets['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Store airline_sentiment in y to use as labels and create an appropriate feature matrix, X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets['airline_sentiment']\n",
    "X = tweets[['compound', 'neg', 'neu', 'pos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit a model of your choice to predict airline_sentient and cross-validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(rf, X, y)\n",
    "# versus the baseline (63%), this is a little weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.30,\n",
    "                                                        random_state=14)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Display the confusion matrix.\n",
    "- What reviews are difficult to identify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conmat = np.array(confusion_matrix(y_test, rf.predict(X_test)))\n",
    "confusion = pd.DataFrame(conmat, index=['negative', 'neutral', 'positive'],\\\n",
    "                     columns=['Pred neg', 'Pred neutral', 'Pred pos'])\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "heat = sns.heatmap(confusion, annot=True, annot_kws={\"size\": 20},cmap='Blues',fmt='g', cbar=False)\n",
    "plt.xticks(rotation=0, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Confusion Matrix\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Print the classification report and discuss the characteristics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does ok with negative tweets (the predominant class) but quite poorly with neutral.\n",
    "\n",
    "To put this in perspective, human concordance, the probability that two people assign the same sentiment to an observation is usually around 70%-80% our baseline is at 63%. Even small increases in accuracy quickly move us towards a theoretical maximum in performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
